{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1r-07xYv9sWeHre529U9YAOSU9ZF600Rm",
      "authorship_tag": "ABX9TyOd1Yw1qoEMqvgtEnXmTM23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virajpvs/Computer_Vision_R-D/blob/main/Copy_of_yolov7_for_custom_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_xDk4bYrrHE"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "from numpy import random\n",
        "import pandas as pd  # for lookup in annotation file\n",
        "import time\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjofZWyV7Zrb",
        "outputId": "9d1d66d8-2dea-4a3d-8639-e91320c48501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/train/images\"\n",
        "# test_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/test/images\"\n",
        "\n",
        "train_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/train\"\n",
        "test_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/test\""
      ],
      "metadata": {
        "id": "BvokNMrf7-p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' data\n",
        "    ├── train\n",
        "    │   ├── images\n",
        "    │   └── labels\n",
        "    ├── test\n",
        "    │   ├── images\n",
        "    │   └── labels\n",
        "    ├── valid\n",
        "    │   ├── images\n",
        "    │   └── labels'''"
      ],
      "metadata": {
        "id": "Y88Paso9jHKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# appling tranforms on datasets\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "qBrRVMRKHo_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageFolder use images with classifed lables\n",
        "# train_dataset = torchvision.datasets.ImageFolder(root= train_dataset_path, transform = train_transform)\n",
        "# test_dataset = torchvision.datasets.ImageFolder(root= test_dataset_path, transform = test_transform)\n",
        "\n",
        "\n",
        "# train_dataset = torchvision.datasets.DatasetFolder(root= train_dataset_path, transform = train_transform)\n",
        "# test_dataset = torchvision.datasets.DatasetFolder(root= test_dataset_path, transform = test_transform)"
      ],
      "metadata": {
        "id": "Ftesm2DHKabK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloaders for train and test sets\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size= 4,shuffle= True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size= 4,shuffle= False)"
      ],
      "metadata": {
        "id": "kTIMO-XlLktC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLODataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect all image files\n",
        "        self.image_files = [f for f in os.listdir(os.path.join(root_dir, 'images')) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, 'images', self.image_files[idx])\n",
        "        label_name = os.path.join(self.root_dir, 'labels', self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "\n",
        "        try:\n",
        "            # Load image\n",
        "            image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "            # Load labels\n",
        "            with open(label_name, 'r') as file:\n",
        "                # Parse YOLO format annotations\n",
        "                label = [list(map(float, line.strip().split())) for line in file.readlines()]\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle errors (e.g., missing files, incorrect formats)\n",
        "            print(f\"Error loading sample {idx}: {e}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "JnFRHr2JLkwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = YOLODataset(root_dir='/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data', transform= transform)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "UGe26H2YLkyY",
        "outputId": "b4240660-1839-4cea-f9ae-7881b5001b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-bf9f9b844d1c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLODataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-537923c6d5e6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, transform)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Collect all image files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/images'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "6G8hqnd67Stp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class YOLODataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        label_name = os.path.join(self.root_dir, 'labels', self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        with open(label_name, 'r') as file:\n",
        "            # Parse YOLO format annotations\n",
        "            # You may need to adjust this based on your YOLO format\n",
        "            # For example, if your YOLO format is [class, x_center, y_center, width, height]\n",
        "            # you can parse it like: label = [list(map(float, line.strip().split())) for line in file.readlines()]\n",
        "            label = [list(map(float, line.strip().split())) for line in file.readlines()]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define your augmentation and normalization transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    # Add more transforms as needed\n",
        "])\n",
        "\n",
        "# Provide the path to your dataset\n",
        "dataset = YOLODataset(root_dir='/content/drive/MyDrive/Colab Notebooks/construction dataset model project/dataset', transform=transform)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate through the dataset using the DataLoader\n",
        "for images, labels in dataloader:\n",
        "    # Your training code here\n",
        "    # 'images' is a tensor of shape (batch_size, channels, height, width)\n",
        "    # 'labels' is a list of lists containing YOLO format annotations for each image in the batch\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "rnHqSvhd4csK",
        "outputId": "d6b4c3bd-17e5-4880-8a1c-c73c08ab764d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c6a7ccb39c84>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Create a DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Iterate through the dataset using the DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "sys.path.append('./')  # to run '$ python *.py' files in subdirectories\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "import models\n",
        "from models.experimental import attempt_load, End2End\n",
        "from utils.activations import Hardswish, SiLU\n",
        "from utils.general import set_logging, check_img_size\n",
        "from utils.torch_utils import select_device\n",
        "from utils.add_nms import RegisterNMS\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', type=str, default='./yolor-csp-c.pt', help='weights path')\n",
        "    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='image size')  # height, width\n",
        "    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n",
        "    parser.add_argument('--dynamic', action='store_true', help='dynamic ONNX axes')\n",
        "    parser.add_argument('--dynamic-batch', action='store_true', help='dynamic batch onnx for tensorrt and onnx-runtime')\n",
        "    parser.add_argument('--grid', action='store_true', help='export Detect() layer grid')\n",
        "    parser.add_argument('--end2end', action='store_true', help='export end2end onnx')\n",
        "    parser.add_argument('--max-wh', type=int, default=None, help='None for tensorrt nms, int value for onnx-runtime nms')\n",
        "    parser.add_argument('--topk-all', type=int, default=100, help='topk objects for every images')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='iou threshold for NMS')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='conf threshold for NMS')\n",
        "    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--simplify', action='store_true', help='simplify onnx model')\n",
        "    parser.add_argument('--include-nms', action='store_true', help='export end2end onnx')\n",
        "    parser.add_argument('--fp16', action='store_true', help='CoreML FP16 half-precision export')\n",
        "    parser.add_argument('--int8', action='store_true', help='CoreML INT8 quantization')\n",
        "    opt = parser.parse_args()\n",
        "    opt.img_size *= 2 if len(opt.img_size) == 1 else 1  # expand\n",
        "    opt.dynamic = opt.dynamic and not opt.end2end\n",
        "    opt.dynamic = False if opt.dynamic_batch else opt.dynamic\n",
        "    print(opt)\n",
        "    set_logging()\n",
        "    t = time.time()\n",
        "\n",
        "    # Load PyTorch model\n",
        "    device = select_device(opt.device)\n",
        "    model = attempt_load(opt.weights, map_location=device)  # load FP32 model\n",
        "    labels = model.names\n",
        "\n",
        "    # Checks\n",
        "    gs = int(max(model.stride))  # grid size (max stride)\n",
        "    opt.img_size = [check_img_size(x, gs) for x in opt.img_size]  # verify img_size are gs-multiples\n",
        "\n",
        "    # Input\n",
        "    img = torch.zeros(opt.batch_size, 3, *opt.img_size).to(device)  # image size(1,3,320,192) iDetection\n",
        "\n",
        "    # Update model\n",
        "    for k, m in model.named_modules():\n",
        "        m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatibility\n",
        "        if isinstance(m, models.common.Conv):  # assign export-friendly activations\n",
        "            if isinstance(m.act, nn.Hardswish):\n",
        "                m.act = Hardswish()\n",
        "            elif isinstance(m.act, nn.SiLU):\n",
        "                m.act = SiLU()\n",
        "        # elif isinstance(m, models.yolo.Detect):\n",
        "        #     m.forward = m.forward_export  # assign forward (optional)\n",
        "    model.model[-1].export = not opt.grid  # set Detect() layer grid export\n",
        "    y = model(img)  # dry run\n",
        "    if opt.include_nms:\n",
        "        model.model[-1].include_nms = True\n",
        "        y = None\n",
        "\n",
        "    # TorchScript export\n",
        "    try:\n",
        "        print('\\nStarting TorchScript export with torch %s...' % torch.__version__)\n",
        "        f = opt.weights.replace('.pt', '.torchscript.pt')  # filename\n",
        "        ts = torch.jit.trace(model, img, strict=False)\n",
        "        ts.save(f)\n",
        "        print('TorchScript export success, saved as %s' % f)\n",
        "    except Exception as e:\n",
        "        print('TorchScript export failure: %s' % e)\n",
        "\n",
        "    # CoreML export\n",
        "    try:\n",
        "        import coremltools as ct\n",
        "\n",
        "        print('\\nStarting CoreML export with coremltools %s...' % ct.__version__)\n",
        "        # convert model from torchscript and apply pixel scaling as per detect.py\n",
        "        ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0])])\n",
        "        bits, mode = (8, 'kmeans_lut') if opt.int8 else (16, 'linear') if opt.fp16 else (32, None)\n",
        "        if bits < 32:\n",
        "            if sys.platform.lower() == 'darwin':  # quantization only supported on macOS\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "                    ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n",
        "            else:\n",
        "                print('quantization only supported on macOS, skipping...')\n",
        "\n",
        "        f = opt.weights.replace('.pt', '.mlmodel')  # filename\n",
        "        ct_model.save(f)\n",
        "        print('CoreML export success, saved as %s' % f)\n",
        "    except Exception as e:\n",
        "        print('CoreML export failure: %s' % e)\n",
        "\n",
        "    # TorchScript-Lite export\n",
        "    try:\n",
        "        print('\\nStarting TorchScript-Lite export with torch %s...' % torch.__version__)\n",
        "        f = opt.weights.replace('.pt', '.torchscript.ptl')  # filename\n",
        "        tsl = torch.jit.trace(model, img, strict=False)\n",
        "        tsl = optimize_for_mobile(tsl)\n",
        "        tsl._save_for_lite_interpreter(f)\n",
        "        print('TorchScript-Lite export success, saved as %s' % f)\n",
        "    except Exception as e:\n",
        "        print('TorchScript-Lite export failure: %s' % e)\n",
        "\n",
        "    # ONNX export\n",
        "    try:\n",
        "        import onnx\n",
        "\n",
        "        print('\\nStarting ONNX export with onnx %s...' % onnx.__version__)\n",
        "        f = opt.weights.replace('.pt', '.onnx')  # filename\n",
        "        model.eval()\n",
        "        output_names = ['classes', 'boxes'] if y is None else ['output']\n",
        "        dynamic_axes = None\n",
        "        if opt.dynamic:\n",
        "            dynamic_axes = {'images': {0: 'batch', 2: 'height', 3: 'width'},  # size(1,3,640,640)\n",
        "             'output': {0: 'batch', 2: 'y', 3: 'x'}}\n",
        "        if opt.dynamic_batch:\n",
        "            opt.batch_size = 'batch'\n",
        "            dynamic_axes = {\n",
        "                'images': {\n",
        "                    0: 'batch',\n",
        "                }, }\n",
        "            if opt.end2end and opt.max_wh is None:\n",
        "                output_axes = {\n",
        "                    'num_dets': {0: 'batch'},\n",
        "                    'det_boxes': {0: 'batch'},\n",
        "                    'det_scores': {0: 'batch'},\n",
        "                    'det_classes': {0: 'batch'},\n",
        "                }\n",
        "            else:\n",
        "                output_axes = {\n",
        "                    'output': {0: 'batch'},\n",
        "                }\n",
        "            dynamic_axes.update(output_axes)\n",
        "        if opt.grid:\n",
        "            if opt.end2end:\n",
        "                print('\\nStarting export end2end onnx model for %s...' % 'TensorRT' if opt.max_wh is None else 'onnxruntime')\n",
        "                model = End2End(model,opt.topk_all,opt.iou_thres,opt.conf_thres,opt.max_wh,device,len(labels))\n",
        "                if opt.end2end and opt.max_wh is None:\n",
        "                    output_names = ['num_dets', 'det_boxes', 'det_scores', 'det_classes']\n",
        "                    shapes = [opt.batch_size, 1, opt.batch_size, opt.topk_all, 4,\n",
        "                              opt.batch_size, opt.topk_all, opt.batch_size, opt.topk_all]\n",
        "                else:\n",
        "                    output_names = ['output']\n",
        "            else:\n",
        "                model.model[-1].concat = True\n",
        "\n",
        "        torch.onnx.export(model, img, f, verbose=False, opset_version=12, input_names=['images'],\n",
        "                          output_names=output_names,\n",
        "                          dynamic_axes=dynamic_axes)\n",
        "\n",
        "        # Checks\n",
        "        onnx_model = onnx.load(f)  # load onnx model\n",
        "        onnx.checker.check_model(onnx_model)  # check onnx model\n",
        "\n",
        "        if opt.end2end and opt.max_wh is None:\n",
        "            for i in onnx_model.graph.output:\n",
        "                for j in i.type.tensor_type.shape.dim:\n",
        "                    j.dim_param = str(shapes.pop(0))\n",
        "\n",
        "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
        "\n",
        "        # # Metadata\n",
        "        # d = {'stride': int(max(model.stride))}\n",
        "        # for k, v in d.items():\n",
        "        #     meta = onnx_model.metadata_props.add()\n",
        "        #     meta.key, meta.value = k, str(v)\n",
        "        # onnx.save(onnx_model, f)\n",
        "\n",
        "        if opt.simplify:\n",
        "            try:\n",
        "                import onnxsim\n",
        "\n",
        "                print('\\nStarting to simplify ONNX...')\n",
        "                onnx_model, check = onnxsim.simplify(onnx_model)\n",
        "                assert check, 'assert check failed'\n",
        "            except Exception as e:\n",
        "                print(f'Simplifier failure: {e}')\n",
        "\n",
        "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
        "        onnx.save(onnx_model,f)\n",
        "        print('ONNX export success, saved as %s' % f)\n",
        "\n",
        "        if opt.include_nms:\n",
        "            print('Registering NMS plugin for ONNX...')\n",
        "            mo = RegisterNMS(f)\n",
        "            mo.register_nms()\n",
        "            mo.save(f)\n",
        "\n",
        "    except Exception as e:\n",
        "        print('ONNX export failure: %s' % e)\n",
        "\n",
        "    # Finish\n",
        "    print('\\nExport complete (%.2fs). Visualize with https://github.com/lutzroeder/netron.' % (time.time() - t))\n"
      ],
      "metadata": {
        "id": "SBmSPA60c56V",
        "outputId": "9ce48f7c-2119-48a5-d4c8-0aaadc15ecd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ceb9e5188a94>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobile_optimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimize_for_mobile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnd2End\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHardswish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSiLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def increment_path(path, exist_ok=True, sep=''):\n",
        "    # Increment path, i.e. runs/exp --> runs/exp{sep}0, runs/exp{sep}1 etc.\n",
        "    path = Path(path)  # os-agnostic\n",
        "    if (path.exists() and exist_ok) or (not path.exists()):\n",
        "        return str(path)\n",
        "    else:\n",
        "        dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n",
        "        matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n",
        "        i = [int(m.groups()[0]) for m in matches if m]  # indices\n",
        "        n = max(i) + 1 if i else 2  # increment number\n",
        "        return f\"{path}{sep}{n}\"  # update path\n"
      ],
      "metadata": {
        "id": "eY7ed3OLcikc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "7rFZDVNUcHkJ",
        "outputId": "125b625a-88a6-47f9-e23d-8e2985f78ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-15d660d279b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# increment run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'labels'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msave_txt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  opt = parser.parse_args()   this is from export.py"
      ],
      "metadata": {
        "id": "1Hl0UL_1sm5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}