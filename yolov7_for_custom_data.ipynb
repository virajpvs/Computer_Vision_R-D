{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2J43vI3+JNu4irACCC47O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virajpvs/Computer_Vision_R-D/blob/main/yolov7_for_custom_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a_xDk4bYrrHE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from numpy import random\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjofZWyV7Zrb",
        "outputId": "0a7d032e-d59c-4fbb-ce9e-40470272af03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/train/images\"\n",
        "# test_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/test/images\"\n",
        "\n",
        "train_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/train\"\n",
        "test_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/test\"\n",
        "valid_dataset_path = \"/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/valid\""
      ],
      "metadata": {
        "id": "BvokNMrf7-p4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' data\n",
        "    ├── train\n",
        "    │   ├── images\n",
        "    │   └── labels\n",
        "    ├── test\n",
        "    │   ├── images\n",
        "    │   └── labels\n",
        "    ├── valid\n",
        "    │   ├── images\n",
        "    │   └── labels'''"
      ],
      "metadata": {
        "id": "Y88Paso9jHKY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7c20f3cc-6c1d-4f30-b0d8-f9bb895ae675"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' data\\n    ├── train\\n    │   ├── images\\n    │   └── labels\\n    ├── test\\n    │   ├── images\\n    │   └── labels\\n    ├── valid\\n    │   ├── images\\n    │   └── labels'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# appling tranforms on datasets\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "qBrRVMRKHo_y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageFolder use images with classifed lables\n",
        "# train_dataset = torchvision.datasets.ImageFolder(root= train_dataset_path, transform = train_transform)\n",
        "# test_dataset = torchvision.datasets.ImageFolder(root= test_dataset_path, transform = test_transform)\n",
        "\n",
        "# train_dataset = torchvision.datasets.DatasetFolder(root= train_dataset_path, transform = train_transform)\n",
        "# test_dataset = torchvision.datasets.DatasetFolder(root= test_dataset_path, transform = test_transform)"
      ],
      "metadata": {
        "id": "Ftesm2DHKabK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETIhyLHCCaQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect all image files\n",
        "        self.image_files = [f for f in os.listdir(os.path.join(dataset_path, 'images')) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.dataset_path, 'images', self.image_files[idx])\n",
        "        label_name = os.path.join(self.dataset_path, 'labels', self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "\n",
        "        try:\n",
        "            # Load image\n",
        "            image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "            # Load labels\n",
        "            with open(label_name, 'r') as file:\n",
        "                # Parse YOLO format annotations\n",
        "                label = [list(map(float, line.strip().split())) for line in file.readlines()]\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle errors (e.g., missing files, incorrect formats)\n",
        "            print(f\"Error loading sample {idx}: {e}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "JnFRHr2JLkwU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use traing set as dataset_path\n",
        "train_dataset_path = '/content/drive/MyDrive/Colab_Notebooks/construction_dataset_model_project/data/train'\n",
        "train_dataset = CustomDataset(train_dataset_path, transform= train_transform)\n",
        "# .__getitem__()\n"
      ],
      "metadata": {
        "id": "UGe26H2YLkyY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)\n",
        "print(type(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz-nqv9a2HXV",
        "outputId": "39822033-3ae0-44bb-b05c-7fc664e6f7bc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.CustomDataset object at 0x7e2abdbbff10>\n",
            "<class '__main__.CustomDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_dataset:\n",
        "  print(images,labels)\n",
        "  print(type(images),type(labels))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueq_RBeyB5Sq",
        "outputId": "f97b8090-a14d-4c23-ff81-adc33277020b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]) [[8.0, 0.686328125, 0.06666666666666667, 0.06015625, 0.13333333333333333]]\n",
            "<class 'torch.Tensor'> <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloaders for train and test sets\n",
        "batch_size = 1\n",
        "dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True , drop_last = True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size= 4,shuffle= False)"
      ],
      "metadata": {
        "id": "_k4UrVbs6_SK"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,s in enumerate(dataloader):\n",
        "  img = s[0]\n",
        "  print(img.shape)  # shape (batch_size, channels, height, width)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_RfbF80HSbm",
        "outputId": "95c820c8-06e4-4fe1-c29a-cb9d36f92d41"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 640, 640])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataloader:\n",
        "  print(i)\n",
        "  break\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggK7JmSRJ-Tf",
        "outputId": "53425670-e927-4054-8035-7c0ef24f5d3b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), [[tensor([0.], dtype=torch.float64), tensor([0.8756], dtype=torch.float64), tensor([0.4336], dtype=torch.float64), tensor([0.0963], dtype=torch.float64), tensor([0.1528], dtype=torch.float64)], [tensor([0.], dtype=torch.float64), tensor([0.7952], dtype=torch.float64), tensor([0.5991], dtype=torch.float64), tensor([0.1925], dtype=torch.float64), tensor([0.1811], dtype=torch.float64)], [tensor([10.], dtype=torch.float64), tensor([0.5332], dtype=torch.float64), tensor([0.7441], dtype=torch.float64), tensor([0.0160], dtype=torch.float64), tensor([0.0657], dtype=torch.float64)], [tensor([16.], dtype=torch.float64), tensor([0.0262], dtype=torch.float64), tensor([0.4459], dtype=torch.float64), tensor([0.0402], dtype=torch.float64), tensor([0.0639], dtype=torch.float64)], [tensor([10.], dtype=torch.float64), tensor([0.7886], dtype=torch.float64), tensor([0.4646], dtype=torch.float64), tensor([0.0049], dtype=torch.float64), tensor([0.0187], dtype=torch.float64)], [tensor([11.], dtype=torch.float64), tensor([0.7313], dtype=torch.float64), tensor([0.4886], dtype=torch.float64), tensor([0.0071], dtype=torch.float64), tensor([0.0211], dtype=torch.float64)], [tensor([2.], dtype=torch.float64), tensor([0.7333], dtype=torch.float64), tensor([0.4734], dtype=torch.float64), tensor([0.0047], dtype=torch.float64), tensor([0.0061], dtype=torch.float64)], [tensor([8.], dtype=torch.float64), tensor([0.7330], dtype=torch.float64), tensor([0.4928], dtype=torch.float64), tensor([0.0106], dtype=torch.float64), tensor([0.0449], dtype=torch.float64)], [tensor([2.], dtype=torch.float64), tensor([0.4149], dtype=torch.float64), tensor([0.4383], dtype=torch.float64), tensor([0.0057], dtype=torch.float64), tensor([0.0049], dtype=torch.float64)], [tensor([2.], dtype=torch.float64), tensor([0.4089], dtype=torch.float64), tensor([0.4426], dtype=torch.float64), tensor([0.0054], dtype=torch.float64), tensor([0.0075], dtype=torch.float64)], [tensor([11.], dtype=torch.float64), tensor([0.4155], dtype=torch.float64), tensor([0.4507], dtype=torch.float64), tensor([0.0063], dtype=torch.float64), tensor([0.0123], dtype=torch.float64)], [tensor([11.], dtype=torch.float64), tensor([0.4083], dtype=torch.float64), tensor([0.4562], dtype=torch.float64), tensor([0.0082], dtype=torch.float64), tensor([0.0191], dtype=torch.float64)], [tensor([8.], dtype=torch.float64), tensor([0.4077], dtype=torch.float64), tensor([0.4571], dtype=torch.float64), tensor([0.0086], dtype=torch.float64), tensor([0.0373], dtype=torch.float64)], [tensor([8.], dtype=torch.float64), tensor([0.4174], dtype=torch.float64), tensor([0.4572], dtype=torch.float64), tensor([0.0111], dtype=torch.float64), tensor([0.0435], dtype=torch.float64)]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "sys.path.append('./')  # to run '$ python *.py' files in subdirectories\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "import models\n",
        "from models.experimental import attempt_load, End2End\n",
        "from utils.activations import Hardswish, SiLU\n",
        "from utils.general import set_logging, check_img_size\n",
        "from utils.torch_utils import select_device\n",
        "from utils.add_nms import RegisterNMS\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', type=str, default='./yolor-csp-c.pt', help='weights path')\n",
        "    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='image size')  # height, width\n",
        "    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n",
        "    parser.add_argument('--dynamic', action='store_true', help='dynamic ONNX axes')\n",
        "    parser.add_argument('--dynamic-batch', action='store_true', help='dynamic batch onnx for tensorrt and onnx-runtime')\n",
        "    parser.add_argument('--grid', action='store_true', help='export Detect() layer grid')\n",
        "    parser.add_argument('--end2end', action='store_true', help='export end2end onnx')\n",
        "    parser.add_argument('--max-wh', type=int, default=None, help='None for tensorrt nms, int value for onnx-runtime nms')\n",
        "    parser.add_argument('--topk-all', type=int, default=100, help='topk objects for every images')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='iou threshold for NMS')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='conf threshold for NMS')\n",
        "    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--simplify', action='store_true', help='simplify onnx model')\n",
        "    parser.add_argument('--include-nms', action='store_true', help='export end2end onnx')\n",
        "    parser.add_argument('--fp16', action='store_true', help='CoreML FP16 half-precision export')\n",
        "    parser.add_argument('--int8', action='store_true', help='CoreML INT8 quantization')\n",
        "    opt = parser.parse_args()\n",
        "    opt.img_size *= 2 if len(opt.img_size) == 1 else 1  # expand\n",
        "    opt.dynamic = opt.dynamic and not opt.end2end\n",
        "    opt.dynamic = False if opt.dynamic_batch else opt.dynamic\n",
        "    print(opt)\n",
        "    set_logging()\n",
        "    t = time.time()\n",
        "\n",
        "    # Load PyTorch model\n",
        "    device = select_device(opt.device)\n",
        "    model = attempt_load(opt.weights, map_location=device)  # load FP32 model\n",
        "    labels = model.names\n",
        "\n",
        "    # Checks\n",
        "    gs = int(max(model.stride))  # grid size (max stride)\n",
        "    opt.img_size = [check_img_size(x, gs) for x in opt.img_size]  # verify img_size are gs-multiples\n",
        "\n",
        "    # Input\n",
        "    img = torch.zeros(opt.batch_size, 3, *opt.img_size).to(device)  # image size(1,3,320,192) iDetection\n",
        "\n",
        "    # Update model\n",
        "    for k, m in model.named_modules():\n",
        "        m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatibility\n",
        "        if isinstance(m, models.common.Conv):  # assign export-friendly activations\n",
        "            if isinstance(m.act, nn.Hardswish):\n",
        "                m.act = Hardswish()\n",
        "            elif isinstance(m.act, nn.SiLU):\n",
        "                m.act = SiLU()\n",
        "        # elif isinstance(m, models.yolo.Detect):\n",
        "        #     m.forward = m.forward_export  # assign forward (optional)\n",
        "    model.model[-1].export = not opt.grid  # set Detect() layer grid export\n",
        "    y = model(img)  # dry run\n",
        "    if opt.include_nms:\n",
        "        model.model[-1].include_nms = True\n",
        "        y = None\n",
        "\n",
        "    # TorchScript export\n",
        "    try:\n",
        "        print('\\nStarting TorchScript export with torch %s...' % torch.__version__)\n",
        "        f = opt.weights.replace('.pt', '.torchscript.pt')  # filename\n",
        "        ts = torch.jit.trace(model, img, strict=False)\n",
        "        ts.save(f)\n",
        "        print('TorchScript export success, saved as %s' % f)\n",
        "    except Exception as e:\n",
        "        print('TorchScript export failure: %s' % e)\n",
        "\n",
        "    # CoreML export\n",
        "    try:\n",
        "        import coremltools as ct\n",
        "\n",
        "        print('\\nStarting CoreML export with coremltools %s...' % ct.__version__)\n",
        "        # convert model from torchscript and apply pixel scaling as per detect.py\n",
        "        ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0])])\n",
        "        bits, mode = (8, 'kmeans_lut') if opt.int8 else (16, 'linear') if opt.fp16 else (32, None)\n",
        "        if bits < 32:\n",
        "            if sys.platform.lower() == 'darwin':  # quantization only supported on macOS\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
        "                    ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n",
        "            else:\n",
        "                print('quantization only supported on macOS, skipping...')\n",
        "\n",
        "        f = opt.weights.replace('.pt', '.mlmodel')  # filename\n",
        "        ct_model.save(f)\n",
        "        print('CoreML export success, saved as %s' % f)\n",
        "    except Exception as e:\n",
        "        print('CoreML export failure: %s' % e)\n",
        "\n",
        "    # TorchScript-Lite export\n",
        "    try:\n",
        "        print('\\nStarting TorchScript-Lite export with torch %s...' % torch.__version__)\n",
        "        f = opt.weights.replace('.pt', '.torchscript.ptl')  # filename\n",
        "        tsl = torch.jit.trace(model, img, strict=False)\n",
        "        tsl = optimize_for_mobile(tsl)\n",
        "        tsl._save_for_lite_interpreter(f)\n",
        "        print('TorchScript-Lite export success, saved as %s' % f)\n",
        "    except Exception as e:\n",
        "        print('TorchScript-Lite export failure: %s' % e)\n",
        "\n",
        "    # ONNX export\n",
        "    try:\n",
        "        import onnx\n",
        "\n",
        "        print('\\nStarting ONNX export with onnx %s...' % onnx.__version__)\n",
        "        f = opt.weights.replace('.pt', '.onnx')  # filename\n",
        "        model.eval()\n",
        "        output_names = ['classes', 'boxes'] if y is None else ['output']\n",
        "        dynamic_axes = None\n",
        "        if opt.dynamic:\n",
        "            dynamic_axes = {'images': {0: 'batch', 2: 'height', 3: 'width'},  # size(1,3,640,640)\n",
        "             'output': {0: 'batch', 2: 'y', 3: 'x'}}\n",
        "        if opt.dynamic_batch:\n",
        "            opt.batch_size = 'batch'\n",
        "            dynamic_axes = {\n",
        "                'images': {\n",
        "                    0: 'batch',\n",
        "                }, }\n",
        "            if opt.end2end and opt.max_wh is None:\n",
        "                output_axes = {\n",
        "                    'num_dets': {0: 'batch'},\n",
        "                    'det_boxes': {0: 'batch'},\n",
        "                    'det_scores': {0: 'batch'},\n",
        "                    'det_classes': {0: 'batch'},\n",
        "                }\n",
        "            else:\n",
        "                output_axes = {\n",
        "                    'output': {0: 'batch'},\n",
        "                }\n",
        "            dynamic_axes.update(output_axes)\n",
        "        if opt.grid:\n",
        "            if opt.end2end:\n",
        "                print('\\nStarting export end2end onnx model for %s...' % 'TensorRT' if opt.max_wh is None else 'onnxruntime')\n",
        "                model = End2End(model,opt.topk_all,opt.iou_thres,opt.conf_thres,opt.max_wh,device,len(labels))\n",
        "                if opt.end2end and opt.max_wh is None:\n",
        "                    output_names = ['num_dets', 'det_boxes', 'det_scores', 'det_classes']\n",
        "                    shapes = [opt.batch_size, 1, opt.batch_size, opt.topk_all, 4,\n",
        "                              opt.batch_size, opt.topk_all, opt.batch_size, opt.topk_all]\n",
        "                else:\n",
        "                    output_names = ['output']\n",
        "            else:\n",
        "                model.model[-1].concat = True\n",
        "\n",
        "        torch.onnx.export(model, img, f, verbose=False, opset_version=12, input_names=['images'],\n",
        "                          output_names=output_names,\n",
        "                          dynamic_axes=dynamic_axes)\n",
        "\n",
        "        # Checks\n",
        "        onnx_model = onnx.load(f)  # load onnx model\n",
        "        onnx.checker.check_model(onnx_model)  # check onnx model\n",
        "\n",
        "        if opt.end2end and opt.max_wh is None:\n",
        "            for i in onnx_model.graph.output:\n",
        "                for j in i.type.tensor_type.shape.dim:\n",
        "                    j.dim_param = str(shapes.pop(0))\n",
        "\n",
        "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
        "\n",
        "        # # Metadata\n",
        "        # d = {'stride': int(max(model.stride))}\n",
        "        # for k, v in d.items():\n",
        "        #     meta = onnx_model.metadata_props.add()\n",
        "        #     meta.key, meta.value = k, str(v)\n",
        "        # onnx.save(onnx_model, f)\n",
        "\n",
        "        if opt.simplify:\n",
        "            try:\n",
        "                import onnxsim\n",
        "\n",
        "                print('\\nStarting to simplify ONNX...')\n",
        "                onnx_model, check = onnxsim.simplify(onnx_model)\n",
        "                assert check, 'assert check failed'\n",
        "            except Exception as e:\n",
        "                print(f'Simplifier failure: {e}')\n",
        "\n",
        "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
        "        onnx.save(onnx_model,f)\n",
        "        print('ONNX export success, saved as %s' % f)\n",
        "\n",
        "        if opt.include_nms:\n",
        "            print('Registering NMS plugin for ONNX...')\n",
        "            mo = RegisterNMS(f)\n",
        "            mo.register_nms()\n",
        "            mo.save(f)\n",
        "\n",
        "    except Exception as e:\n",
        "        print('ONNX export failure: %s' % e)\n",
        "\n",
        "    # Finish\n",
        "    print('\\nExport complete (%.2fs). Visualize with https://github.com/lutzroeder/netron.' % (time.time() - t))\n"
      ],
      "metadata": {
        "id": "SBmSPA60c56V",
        "outputId": "9ce48f7c-2119-48a5-d4c8-0aaadc15ecd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ceb9e5188a94>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobile_optimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimize_for_mobile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnd2End\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHardswish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSiLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  opt = parser.parse_args()   this is from export.py"
      ],
      "metadata": {
        "id": "1Hl0UL_1sm5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}